{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "IEyDAQDnvFo1",
        "outputId": "9a32bdab-de62-4881-dddb-bc188d3933a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">186624</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,888,000</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚             \u001b[38;5;34m896\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m18,496\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m186624\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚      \u001b[38;5;34m23,888,000\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   â”‚             \u001b[38;5;34m258\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,907,650</span> (91.20 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,907,650\u001b[0m (91.20 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,907,650</span> (91.20 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,907,650\u001b[0m (91.20 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Crop Classification Model (Rice or Cotton)\n",
        "crop_model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')  # 2 classes: Rice, Cotton\n",
        "])\n",
        "\n",
        "crop_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "crop_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KysUKqe1v90y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDlyru_YwB77",
        "outputId": "0931192f-b9e5-4a3c-950f-aa4c463ce1cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtfhBnk5wJQ1",
        "outputId": "a3956df1-f8cf-457c-d604-ad7922a60251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7171 images belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/Dataset Crop',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-feJA7JtwTAH",
        "outputId": "9be68817-dfdb-40b9-cf73-5a5f851ef601"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'.ipynb_checkpoints': 0, 'Crop': 1, 'deficiency': 2}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_set.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTWC4zC2wXDa"
      },
      "outputs": [],
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwKj68ZIwZ6k",
        "outputId": "4802ab77-593c-4c49-9f49-4e0f4dcef7ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBk4YHq7wdIQ"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5CTx-Vuwflj"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpSo2bAdwh-o"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lKpLH4QwncN"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kneklf7Hwpx6"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cySqXhPwp-o"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=64, activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmRNR8VgwwYs"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsJlQ_51wwuU"
      },
      "outputs": [],
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmHHcr-Qw34s",
        "outputId": "00754797-7277-48e8-c34c-d5c0d7c08ea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7171 images belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "testing_set = test_datagen.flow_from_directory('/content/drive/MyDrive/Dataset Crop',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypbod4YOxLO7",
        "outputId": "697e0e90-fb52-46f0-b2a1-86d5cfec1441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 12041 images belonging to 4 classes.\n",
            "Found 12041 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define ImageDataGenerator for training and validation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, horizontal_flip=True)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load Training Data\n",
        "train_crop_data = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Dataset Crop/Crop/train',  # Path to training dataset\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load Validation Data\n",
        "val_crop_data = val_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Dataset Crop/Crop/validation',  # Path to validation dataset\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZvEsWc218EW",
        "outputId": "e0a7a2f6-c126-4181-d65b-b44084c79b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "4N0g1J0Sk6Rh",
        "outputId": "53aa2dd8-5008-4a38-de44-5ca42d239892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 9634 images belonging to 4 classes.\n",
            "Found 2407 images belonging to 4 classes.\n",
            "Found 9639 images belonging to 17 classes.\n",
            "Found 2402 images belonging to 17 classes.\n",
            "\n",
            "Training Crop Model...\n",
            "Epoch 1/3\n",
            "\u001b[1m 69/603\u001b[0m \u001b[32mâ”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:04:26\u001b[0m 7s/step - accuracy: 0.7955 - loss: 1.2943"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4cacd02f0007>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining Crop Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mcrop_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mcrop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrop_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_crop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ğŸ”¹ Reduced to 3 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Train Deficiency Model with fewer epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Set optimized image size and batch size\n",
        "IMG_SIZE = (96, 96)  # Optimized image size\n",
        "BATCH_SIZE = 16  # Reduced batch size\n",
        "\n",
        "# Dataset paths\n",
        "CROP_TRAIN_DIR = \"/content/drive/MyDrive/Dataset Crop/Crop/train\"\n",
        "CROP_TEST_DIR = \"/content/drive/MyDrive/Dataset Crop/Crop/validation\"\n",
        "\n",
        "DEFICIENCY_TRAIN_DIR = \"/content/drive/MyDrive/Dataset Crop/deficiency/train\"\n",
        "DEFICIENCY_TEST_DIR = \"/content/drive/MyDrive/Dataset Crop/deficiency/validation\"\n",
        "\n",
        "# Image Data Generators (rescale images)\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Load Crop Classification Dataset\n",
        "crop_train = datagen.flow_from_directory(\n",
        "    CROP_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='training')\n",
        "crop_val = datagen.flow_from_directory(\n",
        "    CROP_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='validation')\n",
        "\n",
        "# Load Deficiency Classification Dataset\n",
        "deficiency_train = datagen.flow_from_directory(\n",
        "    DEFICIENCY_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='training')\n",
        "deficiency_val = datagen.flow_from_directory(\n",
        "    DEFICIENCY_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='validation')\n",
        "\n",
        "# Function to create a model using MobileNetV2\n",
        "def create_model(num_classes):\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
        "    base_model.trainable = False  # Freeze base model layers\n",
        "\n",
        "    x = Flatten()(base_model.output)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# ModelCheckpoint to save the best model during training\n",
        "checkpoint_crop = ModelCheckpoint(\n",
        "    \"crop_model_best.h5\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "checkpoint_deficiency = ModelCheckpoint(\n",
        "    \"deficiency_model_best.h5\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "# Train Crop Model with fewer epochs\n",
        "print(\"\\nTraining Crop Model...\")\n",
        "crop_model = create_model(len(crop_train.class_indices))\n",
        "crop_model.fit(crop_train, validation_data=crop_val, epochs=3, callbacks=[checkpoint_crop])  # ğŸ”¹ Reduced to 3 epochs\n",
        "\n",
        "# Train Deficiency Model with fewer epochs\n",
        "print(\"\\nTraining Deficiency Model...\")\n",
        "deficiency_model = create_model(len(deficiency_train.class_indices))\n",
        "deficiency_model.fit(deficiency_train, validation_data=deficiency_val, epochs=3, callbacks=[checkpoint_deficiency])  # ğŸ”¹ Reduced to 3 epochs\n",
        "\n",
        "print(\"\\nâœ… Training Completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "DFsr-a4G7XFg",
        "outputId": "480b0436-ed7e-462e-9dae-d36172715336"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'crop_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4511f7c39867>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Test with an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mclassify_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Dataset Crop/Crop/validation/Rice/Potassium(K)/untitled-1 - Copy - Copy.JPG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-4511f7c39867>\u001b[0m in \u001b[0;36mclassify_image\u001b[0;34m(img_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcrop_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcrop_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcrop_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcrop_class\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# âœ… Crop mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Crop Identified: {crop_label}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'crop_train' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Load trained models\n",
        "crop_model = load_model(\"/content/drive/MyDrive/crop_model.h5\")\n",
        "deficiency_model = load_model(\"/content/drive/MyDrive/deficiency_model.h5\")\n",
        "\n",
        "# Load and preprocess the test image\n",
        "def classify_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(96, 96))  # âœ… Match training size\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Predict Crop\n",
        "    crop_pred = crop_model.predict(img_array)\n",
        "    crop_class = np.argmax(crop_pred)\n",
        "    crop_label = list(crop_train.class_indices.keys())[crop_class]  # âœ… Crop mapping\n",
        "\n",
        "    print(f\"Crop Identified: {crop_label}\")\n",
        "\n",
        "    # Predict Deficiency\n",
        "    deficiency_pred = deficiency_model.predict(img_array)\n",
        "    deficiency_class = np.argmax(deficiency_pred)\n",
        "    deficiency_label = list(deficiency_train.class_indices.keys())[deficiency_class]  # âœ… Deficiency mapping\n",
        "\n",
        "    print(f\"Deficiency Identified: {deficiency_label}\")\n",
        "\n",
        "# Test with an image\n",
        "classify_image(\"/content/drive/MyDrive/Dataset Crop/Crop/validation/Rice/Potassium(K)/untitled-1 - Copy - Copy.JPG\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5VviZIkZlZN",
        "outputId": "0c8f0170-7735-4621-f2af-38ef49b13597"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O88QZCzjV54M",
        "outputId": "703ab5c2-e22e-45c7-eef2-6287a6aad2ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 9634 images belonging to 4 classes.\n",
            "Found 2407 images belonging to 4 classes.\n",
            "Found 9639 images belonging to 17 classes.\n",
            "Found 2402 images belonging to 17 classes.\n",
            "\n",
            "Training Crop Model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 19s/step - accuracy: 0.4706 - loss: 1.5483 - val_accuracy: 0.7312 - val_loss: 0.7094\n",
            "Epoch 2/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 14s/step - accuracy: 0.9224 - loss: 0.2038 - val_accuracy: 0.8125 - val_loss: 0.6512\n",
            "Epoch 3/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 15s/step - accuracy: 0.9436 - loss: 0.2075 - val_accuracy: 0.8250 - val_loss: 0.4645\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Deficiency Model...\n",
            "Epoch 1/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 15s/step - accuracy: 0.1770 - loss: 3.1486 - val_accuracy: 0.6125 - val_loss: 1.3771\n",
            "Epoch 2/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 10s/step - accuracy: 0.6248 - loss: 1.3227 - val_accuracy: 0.7500 - val_loss: 0.8541\n",
            "Epoch 3/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 16s/step - accuracy: 0.7317 - loss: 0.8813 - val_accuracy: 0.8375 - val_loss: 0.5766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Training Completed in under 5 minutes!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set optimized image size and batch size\n",
        "IMG_SIZE = (96, 96)\n",
        "BATCH_SIZE = 32  # Updated batch size for better training performance\n",
        "\n",
        "# Dataset paths (Ensure these paths exist in Drive)\n",
        "CROP_TRAIN_DIR = \"/content/drive/MyDrive/Dataset Crop/Crop/train\"\n",
        "CROP_TEST_DIR = \"/content/drive/MyDrive/Dataset Crop/Crop/validation\"\n",
        "\n",
        "DEFICIENCY_TRAIN_DIR = \"/content/drive/MyDrive/Dataset Crop/deficiency/train\"\n",
        "DEFICIENCY_TEST_DIR = \"/content/drive/MyDrive/Dataset Crop/deficiency/validation\"\n",
        "\n",
        "# Image Data Generators (rescale images)\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Load Crop Classification Dataset\n",
        "crop_train = datagen.flow_from_directory(\n",
        "    CROP_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='training')\n",
        "crop_val = datagen.flow_from_directory(\n",
        "    CROP_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='validation')\n",
        "\n",
        "# Load Deficiency Classification Dataset\n",
        "deficiency_train = datagen.flow_from_directory(\n",
        "    DEFICIENCY_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='training')\n",
        "deficiency_val = datagen.flow_from_directory(\n",
        "    DEFICIENCY_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='validation')\n",
        "\n",
        "# Function to create a model using MobileNetV2\n",
        "def create_model(num_classes):\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
        "    base_model.trainable = False  # Freeze base model layers for fast training\n",
        "\n",
        "    x = GlobalAveragePooling2D()(base_model.output)  # Faster pooling instead of Flatten\n",
        "    x = Dense(128, activation='relu')(x)  # Reduced neurons for faster training\n",
        "    x = Dropout(0.3)(x)  # Optimized dropout\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train Crop Model\n",
        "print(\"\\nTraining Crop Model...\")\n",
        "crop_model = create_model(len(crop_train.class_indices))\n",
        "crop_model.fit(crop_train, validation_data=crop_val, epochs=3, steps_per_epoch=10, validation_steps=5)  # Limited steps\n",
        "crop_model.save(\"/content/drive/MyDrive/crop_model.h5\")\n",
        "\n",
        "# Train Deficiency Model\n",
        "print(\"\\nTraining Deficiency Model...\")\n",
        "deficiency_model = create_model(len(deficiency_train.class_indices))\n",
        "deficiency_model.fit(deficiency_train, validation_data=deficiency_val, epochs=3, steps_per_epoch=10, validation_steps=5)  # Limited steps\n",
        "deficiency_model.save(\"/content/drive/MyDrive/deficiency_model.h5\")\n",
        "\n",
        "print(\"\\nâœ… Training Completed in under 5 minutes!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yW2VAXSZ3Au",
        "outputId": "203ad1e6-65d8-4824-fc2e-30d92624f168"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\n",
            "ğŸŒ¾ **Crop Identified:** Rice\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "âš ï¸ **Deficiency Identified:** Boron\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# âœ… Set the model paths\n",
        "CROP_MODEL_PATH = \"/content/drive/MyDrive/crop_model.h5\"\n",
        "DEFICIENCY_MODEL_PATH = \"/content/drive/MyDrive/deficiency_model.h5\"\n",
        "\n",
        "# âœ… Check if models exist before loading\n",
        "if not os.path.exists(CROP_MODEL_PATH) or not os.path.exists(DEFICIENCY_MODEL_PATH):\n",
        "    raise FileNotFoundError(\"ğŸš¨ Trained models not found! Ensure training was completed successfully.\")\n",
        "\n",
        "# âœ… Load the trained models\n",
        "crop_model = load_model(CROP_MODEL_PATH)\n",
        "deficiency_model = load_model(DEFICIENCY_MODEL_PATH)\n",
        "\n",
        "# âœ… Define the test image path (Make sure the image exists)\n",
        "TEST_IMAGE_PATH = \"/content/drive/MyDrive/Dataset Crop/Crop/train/Rice/Zinc/0004.jpg\"\n",
        "\n",
        "if not os.path.exists(TEST_IMAGE_PATH):\n",
        "    raise FileNotFoundError(\"ğŸš¨ Test image not found! Check the file path.\")\n",
        "\n",
        "# âœ… Load and preprocess the test image\n",
        "def classify_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(96, 96))  # Match training size\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "    # âœ… Predict Crop\n",
        "    crop_pred = crop_model.predict(img_array)\n",
        "    crop_class = np.argmax(crop_pred)\n",
        "    crop_label = list(crop_train.class_indices.keys())[crop_class]  # Get crop name\n",
        "\n",
        "    print(f\"\\nğŸŒ¾ **Crop Identified:** {crop_label}\")\n",
        "\n",
        "    # âœ… Predict Deficiency\n",
        "    deficiency_pred = deficiency_model.predict(img_array)\n",
        "    deficiency_class = np.argmax(deficiency_pred)\n",
        "    deficiency_label = list(deficiency_train.class_indices.keys())[deficiency_class]  # Get deficiency name\n",
        "\n",
        "    print(f\"âš ï¸ **Deficiency Identified:** {deficiency_label}\")\n",
        "\n",
        "# âœ… Test the model output with the image\n",
        "classify_image(TEST_IMAGE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkXEh7Ghb0Pn",
        "outputId": "c116a3f4-7730-454b-dc51-2556f1684499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 9634 images belonging to 4 classes.\n",
            "Found 2407 images belonging to 4 classes.\n",
            "Found 9639 images belonging to 17 classes.\n",
            "Found 2402 images belonging to 17 classes.\n",
            "\n",
            "Training Crop Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 11s/step - accuracy: 0.7314 - loss: 0.6875 - val_accuracy: 0.5562 - val_loss: 3.7101\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 10s/step - accuracy: 0.9741 - loss: 0.0718 - val_accuracy: 0.3625 - val_loss: 10.0159\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 7s/step - accuracy: 0.9565 - loss: 0.1255 - val_accuracy: 0.4219 - val_loss: 13.2008\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 6s/step - accuracy: 0.9530 - loss: 0.1576 - val_accuracy: 0.5250 - val_loss: 5.1427\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 5s/step - accuracy: 0.9656 - loss: 0.1266 - val_accuracy: 0.7031 - val_loss: 4.0199\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 7s/step - accuracy: 0.9616 - loss: 0.0933 - val_accuracy: 0.6562 - val_loss: 5.7608\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 5s/step - accuracy: 0.9859 - loss: 0.0572 - val_accuracy: 0.4313 - val_loss: 11.3277\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 4s/step - accuracy: 0.9772 - loss: 0.1242 - val_accuracy: 0.3469 - val_loss: 13.6745\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 4s/step - accuracy: 0.9771 - loss: 0.1600 - val_accuracy: 0.3906 - val_loss: 21.9969\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4s/step - accuracy: 0.9668 - loss: 0.1499 - val_accuracy: 0.4844 - val_loss: 16.0551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Deficiency Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 5s/step - accuracy: 0.4740 - loss: 1.7612 - val_accuracy: 0.4750 - val_loss: 2.7809\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.8108 - loss: 0.5949 - val_accuracy: 0.3500 - val_loss: 6.8176\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3s/step - accuracy: 0.8501 - loss: 0.4977 - val_accuracy: 0.2250 - val_loss: 10.2773\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - accuracy: 0.8898 - loss: 0.4147 - val_accuracy: 0.2844 - val_loss: 7.4057\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 4s/step - accuracy: 0.8679 - loss: 0.5049 - val_accuracy: 0.4594 - val_loss: 7.4853\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.9255 - loss: 0.2812 - val_accuracy: 0.5688 - val_loss: 4.2118\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8718 - loss: 0.4030 - val_accuracy: 0.4250 - val_loss: 8.3270\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - accuracy: 0.9306 - loss: 0.3028 - val_accuracy: 0.5250 - val_loss: 4.3190\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8845 - loss: 0.3898 - val_accuracy: 0.5125 - val_loss: 4.2511\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8761 - loss: 0.3764 - val_accuracy: 0.4469 - val_loss: 5.3728\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Training Completed Successfully!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set optimized image size and batch size\n",
        "IMG_SIZE = (96, 96)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Dataset paths (Ensure these paths exist in Drive)\n",
        "CROP_TRAIN_DIR = \"/content/drive/MyDrive/Dataset Crop/Crop/train\"\n",
        "CROP_TEST_DIR = \"/content/drive/MyDrive/Dataset Crop/Crop/validation\"\n",
        "\n",
        "DEFICIENCY_TRAIN_DIR = \"/content/drive/MyDrive/Dataset Crop/deficiency/train\"\n",
        "DEFICIENCY_TEST_DIR = \"/content/drive/MyDrive/Dataset Crop/deficiency/validation\"\n",
        "\n",
        "# Image Data Generators (rescale images)\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Load Crop Classification Dataset\n",
        "crop_train = datagen.flow_from_directory(\n",
        "    CROP_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='training')\n",
        "crop_val = datagen.flow_from_directory(\n",
        "    CROP_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='validation')\n",
        "\n",
        "# Load Deficiency Classification Dataset\n",
        "deficiency_train = datagen.flow_from_directory(\n",
        "    DEFICIENCY_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='training')\n",
        "deficiency_val = datagen.flow_from_directory(\n",
        "    DEFICIENCY_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='validation')\n",
        "\n",
        "# Function to create a model using MobileNetV2\n",
        "def create_model(num_classes):\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
        "\n",
        "    # Fine-tune only the last few layers\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:100]:  # Freeze first 100 layers\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train Crop Model\n",
        "print(\"\\nTraining Crop Model...\")\n",
        "crop_model = create_model(len(crop_train.class_indices))\n",
        "crop_model.fit(crop_train, validation_data=crop_val, epochs=10, steps_per_epoch=20, validation_steps=10)  # Increased steps\n",
        "crop_model.save(\"/content/drive/MyDrive/crop_model.h5\")\n",
        "\n",
        "# Train Deficiency Model\n",
        "print(\"\\nTraining Deficiency Model...\")\n",
        "deficiency_model = create_model(len(deficiency_train.class_indices))\n",
        "deficiency_model.fit(deficiency_train, validation_data=deficiency_val, epochs=10, steps_per_epoch=20, validation_steps=10)\n",
        "deficiency_model.save(\"/content/drive/MyDrive/deficiency_model.h5\")\n",
        "\n",
        "# Save class mappings for correct predictions\n",
        "with open(\"/content/drive/MyDrive/crop_class_indices.json\", \"w\") as f:\n",
        "    json.dump(crop_train.class_indices, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/deficiency_class_indices.json\", \"w\") as f:\n",
        "    json.dump(deficiency_train.class_indices, f)\n",
        "\n",
        "print(\"\\nâœ… Training Completed Successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xL7anFBjLx3",
        "outputId": "2b7aabd9-cee9-4959-8ce4-fe5d370b7c57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\n",
            "ğŸŒ± Crop Identified: Rice\n",
            "âš ï¸ Deficiency Identified: Phosphorus(P)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Load trained models\n",
        "crop_model = load_model(\"/content/drive/MyDrive/crop_model.h5\")\n",
        "deficiency_model = load_model(\"/content/drive/MyDrive/deficiency_model.h5\")\n",
        "\n",
        "# Load saved class mappings\n",
        "with open(\"/content/drive/MyDrive/crop_class_indices.json\", \"r\") as f:\n",
        "    crop_classes = json.load(f)\n",
        "crop_classes = {v: k for k, v in crop_classes.items()}  # Reverse mapping\n",
        "\n",
        "with open(\"/content/drive/MyDrive/deficiency_class_indices.json\", \"r\") as f:\n",
        "    deficiency_classes = json.load(f)\n",
        "deficiency_classes = {v: k for k, v in deficiency_classes.items()}  # Reverse mapping\n",
        "\n",
        "# Load and preprocess the test image\n",
        "def classify_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(96, 96))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Predict Crop\n",
        "    crop_pred = crop_model.predict(img_array)\n",
        "    crop_class = np.argmax(crop_pred)\n",
        "    crop_label = crop_classes[crop_class]  # Get correct label\n",
        "\n",
        "    # Predict Deficiency\n",
        "    deficiency_pred = deficiency_model.predict(img_array)\n",
        "    deficiency_class = np.argmax(deficiency_pred)\n",
        "    deficiency_label = deficiency_classes[deficiency_class]  # Get correct label\n",
        "\n",
        "    print(f\"\\nğŸŒ± Crop Identified: {crop_label}\")\n",
        "    print(f\"âš ï¸ Deficiency Identified: {deficiency_label}\")\n",
        "\n",
        "# Test with an image\n",
        "classify_image(\"/content/drive/MyDrive/Dataset Crop/Crop/train/Rice/Phosphorus(P)/IMG_3785 - Copy.JPG\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9WC-v1CbMAO",
        "outputId": "a754237f-be7f-46b1-f621-c0ecca4de7d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Qn2Bn-cu_W",
        "outputId": "7853453d-78a1-4999-c08e-b94d3f6c2a86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/tmp/tmpv0tfcb1d'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name='input_layer_2')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137052488910608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488910800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488913488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488912144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488913104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488912720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488910992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488912912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488914064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488914832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488915408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488916368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488916752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488916560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488916176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488915984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488918288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488914640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488918480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488918096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488920592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488920976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488921360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488921168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488917520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488922512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488922896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488923280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488923088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488919440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488924432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488925008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488923664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488924816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488922128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052488918672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486763920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486762960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486763728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486762768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486764688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486765648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486766032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486765840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486765456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486767184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486767568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486767952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486767760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486763536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486769104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486769488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486769872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486769680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486764304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486771024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486771408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486771792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486771600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486766800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486772944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486773328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486773712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486773520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486768720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486774864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486775248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486775632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486775440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486770640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486776784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486777168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486777552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486777360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486772560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486778704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486774480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486942800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486778320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486776400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486944528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486944912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486945296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486945104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486943760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486946448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486946832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486947216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486947024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486943376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486948368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486948752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486949136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486948944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486944144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486950288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486950672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486951056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486950864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486946064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486952208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486952592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486952976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486952784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486947984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486954128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486954512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486954896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486954704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486949904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486956048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486956432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486956816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486956624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486951824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486957968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486957584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486955664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486958544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486953744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052486957200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476671184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476670224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476670992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476670032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476672720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476673104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476673488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476673296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476670416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476674640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476675024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476675408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476675216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476671568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476676560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476676944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476677328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476677136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476672336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476678480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476678864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476679248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476679056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476674256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476680400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476680784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476681168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476680976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476676176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476682320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476682704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476683088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476682896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476678096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476684240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476684624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476685008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476684816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476680016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476686160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476681936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476850832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476685776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476683856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476851984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476852368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476852752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476852560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476851216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476853904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476854288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476854672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476854480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476850448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476855824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476856208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476856592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476856400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476851600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476857744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476858128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476858512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476858320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476853520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476859664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476860048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476860432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476860240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476855440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476861584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476861968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476862352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476862160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476857360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476863504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476863888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476864272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476864080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476859280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476865424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476865040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476863120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476866000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476861200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052476864656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477015248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477014672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477015440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477014480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477016784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477017168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477017552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477017360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477015632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477018704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477019088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477019472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477019280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477014288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477020624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477021008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477021392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477021200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477016400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477022544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477022928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477023312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477023120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477018320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477024464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477024848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477025232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477025040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477020240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477026384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477026768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477027152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477026960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477022160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477028304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477028688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477029072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477028880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477024080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477030224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477026000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477227664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477029840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477027920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477228816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477229200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477229584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477229392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477228048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477230736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477231120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477231504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477231312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477227280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477232656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477233232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477232272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137052477234000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "âœ… Model converted and saved as TFLite!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load your model from the correct path\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/Saved_Models/crop_model.h5\")\n",
        "\n",
        "# Convert to TFLite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model back to Drive\n",
        "with open(\"/content/drive/MyDrive/Saved_Models/crop_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"âœ… Model converted and saved as TFLite!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JedQgP514A8i",
        "outputId": "eb1e40fb-2662-4ca4-a2bf-864fee9a9d9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 5737 images belonging to 2 classes.\n",
            "Found 1434 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 449ms/step - accuracy: 0.9940 - loss: 0.0191 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 482ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 488ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 442ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 439ms/step - accuracy: 1.0000 - loss: 1.2615e-11 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 488ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 441ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 488ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 486ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 430ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import os\n",
        "\n",
        "# Step 2: Mount Google Drive (if not already done)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 3: Preprocessing the Images\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   validation_split=0.2)  # 80% training, 20% validation\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/Dataset Crop/Crop'  # <--- your updated path\n",
        "\n",
        "# Training set\n",
        "training_set = train_datagen.flow_from_directory(dataset_path,\n",
        "                                                 target_size=(64, 64),\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 subset='training')\n",
        "\n",
        "# Validation set\n",
        "validation_set = train_datagen.flow_from_directory(dataset_path,\n",
        "                                                   target_size=(64, 64),\n",
        "                                                   batch_size=32,\n",
        "                                                   class_mode='categorical',\n",
        "                                                   subset='validation')\n",
        "\n",
        "# Step 4: Build the CNN model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(training_set.num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model\n",
        "\n",
        "model.fit(training_set,\n",
        "          epochs=10,\n",
        "          validation_data=validation_set)\n",
        "\n",
        "# Step 6: Create fertilizer suggestions mapping\n",
        "\n",
        "fertilizer_mapping = {\n",
        "    'Nitrogen Deficiency': 'Use Urea fertilizer',\n",
        "    'Phosphorous Deficiency': 'Use Single Super Phosphate (SSP)',\n",
        "    'Potassium Deficiency': 'Use Muriate of Potash (MOP)',\n",
        "    'Iron Deficiency': 'Use Ferrous Sulphate',\n",
        "    'Zinc Deficiency': 'Use Zinc Sulphate',\n",
        "    # Add more classes if needed\n",
        "}\n",
        "\n",
        "# Step 7: Function to Predict Fertilizer for a New Image\n",
        "\n",
        "def predict_fertilizer(image_path):\n",
        "    img = image.load_img(image_path, target_size=(64, 64))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img /= 255.0  # Normalize\n",
        "\n",
        "    result = model.predict(img)\n",
        "    class_index = np.argmax(result)\n",
        "\n",
        "    class_labels = list(training_set.class_indices.keys())\n",
        "    predicted_class = class_labels[class_index]\n",
        "\n",
        "    fertilizer = fertilizer_mapping.get(predicted_class, \"No fertilizer suggestion available\")\n",
        "\n",
        "    print(f\"\\nDetected Deficiency: {predicted_class}\")\n",
        "    print(f\"Suggested Fertilizer: {fertilizer}\")\n",
        "\n",
        "# Example Usage:\n",
        "# predict_fertilizer('/content/drive/MyDrive/test_leaf.jpg')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(validation_set)\n",
        "print(f\"\\nValidation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Predict classes\n",
        "validation_set.reset()\n",
        "Y_pred = model.predict(validation_set)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "# True classes\n",
        "true_classes = validation_set.classes\n",
        "class_labels = list(validation_set.class_indices.keys())\n",
        "\n",
        "# Always pass all class labels\n",
        "all_class_indices = list(range(len(class_labels)))\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(true_classes, y_pred, labels=all_class_indices, target_names=class_labels, zero_division=0))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\\n\")\n",
        "print(confusion_matrix(true_classes, y_pred, labels=all_class_indices))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igk2mxosAHjf",
        "outputId": "5b10b1d1-31c6-4f4a-f237-aa9378e2ac3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 278ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "\n",
            "Validation Loss: 0.0000\n",
            "Validation Accuracy: 1.0000\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 326ms/step\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       train       1.00      1.00      1.00      1434\n",
            "  validation       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           1.00      1434\n",
            "   macro avg       0.50      0.50      0.50      1434\n",
            "weighted avg       1.00      1.00      1.00      1434\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "\n",
            "[[1434    0]\n",
            " [   0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(validation_set)\n",
        "print(f\"\\nValidation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Predict classes\n",
        "validation_set.reset()\n",
        "Y_pred = model.predict(validation_set)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "# True classes\n",
        "true_classes = validation_set.classes\n",
        "class_labels = list(validation_set.class_indices.keys())\n",
        "\n",
        "# Always pass all class labels\n",
        "all_class_indices = list(range(len(class_labels)))\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(true_classes, y_pred, labels=all_class_indices, target_names=class_labels, zero_division=0))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(true_classes, y_pred, labels=all_class_indices)\n",
        "print(\"\\nConfusion Matrix:\\n\")\n",
        "print(cm)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "aEO5N8v6Bb_n",
        "outputId": "f56a289e-b9f6-4643-83c7-654128f07816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 268ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
            "\n",
            "Validation Loss: 0.0000\n",
            "Validation Accuracy: 1.0000\n",
            "\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 269ms/step\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       train       1.00      1.00      1.00      1434\n",
            "  validation       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           1.00      1434\n",
            "   macro avg       0.50      0.50      0.50      1434\n",
            "weighted avg       1.00      1.00      1.00      1434\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "\n",
            "[[1434    0]\n",
            " [   0    0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xnQAH9CL373d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Load trained models\n",
        "crop_model = load_model(\"/content/drive/MyDrive/crop_model.h5\")\n",
        "deficiency_model = load_model(\"/content/drive/MyDrive/deficiency_model.h5\")\n",
        "\n",
        "# Load saved class mappings\n",
        "with open(\"/content/drive/MyDrive/crop_class_indices.json\", \"r\") as f:\n",
        "    crop_classes = json.load(f)\n",
        "crop_classes = {v: k for k, v in crop_classes.items()}  # Reverse mapping\n",
        "\n",
        "with open(\"/content/drive/MyDrive/deficiency_class_indices.json\", \"r\") as f:\n",
        "    deficiency_classes = json.load(f)\n",
        "deficiency_classes = {v: k for k, v in deficiency_classes.items()}  # Reverse mapping\n",
        "\n",
        "# Load and preprocess the test image\n",
        "def classify_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(96, 96))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Predict Crop\n",
        "    crop_pred = crop_model.predict(img_array)\n",
        "    crop_class = np.argmax(crop_pred)\n",
        "    crop_label = crop_classes[crop_class]  # Get correct label\n",
        "\n",
        "    # Predict Deficiency\n",
        "    deficiency_pred = deficiency_model.predict(img_array)\n",
        "    deficiency_class = np.argmax(deficiency_pred)\n",
        "    deficiency_label = deficiency_classes[deficiency_class]  # Get correct label\n",
        "\n",
        "    print(f\"\\nğŸŒ± Crop Identified: {crop_label}\")\n",
        "    print(f\"âš ï¸ Deficiency Identified: {deficiency_label}\")\n",
        "\n",
        "# Test with an image\n",
        "classify_image(\"/content/drive/MyDrive/Dataset Crop/deficiency/train/Blight/Corn_Blight (10).jpg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y7SwVaXBv9z",
        "outputId": "5e970c4e-c580-40ff-ff8b-7217122d3ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\n",
            "ğŸŒ± Crop Identified: Mazie\n",
            "âš ï¸ Deficiency Identified: Potassium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set optimized image size and batch size\n",
        "IMG_SIZE = (96, 96)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Dataset paths (Ensure these paths exist in Drive)\n",
        "CROP_TRAIN_DIR = \"/content/drive/MyDrive/Dataset Crop/Crop/train\"\n",
        "CROP_TEST_DIR = \"/content/drive/MyDrive/Dataset Crop/Crop/validation\"\n",
        "\n",
        "DEFICIENCY_TRAIN_DIR = \"/content/drive/MyDrive/Dataset Crop/deficiency/train\"\n",
        "DEFICIENCY_TEST_DIR = \"/content/drive/MyDrive/Dataset Crop/deficiency/validation\"\n",
        "\n",
        "# Image Data Generators (rescale images)\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Load Crop Classification Dataset\n",
        "crop_train = datagen.flow_from_directory(\n",
        "    CROP_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='training')\n",
        "crop_val = datagen.flow_from_directory(\n",
        "    CROP_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='validation')\n",
        "\n",
        "# Load Deficiency Classification Dataset\n",
        "deficiency_train = datagen.flow_from_directory(\n",
        "    DEFICIENCY_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='training')\n",
        "deficiency_val = datagen.flow_from_directory(\n",
        "    DEFICIENCY_TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', subset='validation')\n",
        "\n",
        "# Function to create a model using MobileNetV2\n",
        "def create_model(num_classes):\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
        "\n",
        "    # Fine-tune only the last few layers\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:100]:  # Freeze first 100 layers\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train Crop Model\n",
        "print(\"\\nTraining Crop Model...\")\n",
        "crop_model = create_model(len(crop_train.class_indices))\n",
        "crop_model.fit(crop_train, validation_data=crop_val, epochs=10, steps_per_epoch=20, validation_steps=10)  # Increased steps\n",
        "crop_model.save(\"/content/drive/MyDrive/crop_model.h5\")\n",
        "\n",
        "# Train Deficiency Model\n",
        "print(\"\\nTraining Deficiency Model...\")\n",
        "deficiency_model = create_model(len(deficiency_train.class_indices))\n",
        "deficiency_model.fit(deficiency_train, validation_data=deficiency_val, epochs=10, steps_per_epoch=20, validation_steps=10)\n",
        "deficiency_model.save(\"/content/drive/MyDrive/deficiency_model.h5\")\n",
        "\n",
        "# Save class mappings for correct predictions\n",
        "with open(\"/content/drive/MyDrive/crop_class_indices.json\", \"w\") as f:\n",
        "    json.dump(crop_train.class_indices, f)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/deficiency_class_indices.json\", \"w\") as f:\n",
        "    json.dump(deficiency_train.class_indices, f)\n",
        "\n",
        "print(\"\\nâœ… Training Completed Successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g1NjSEL3-Po",
        "outputId": "5ee32944-a642-452a-c49e-fc3895d4f60f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 5738 images belonging to 2 classes.\n",
            "Found 1433 images belonging to 2 classes.\n",
            "Found 5740 images belonging to 7 classes.\n",
            "Found 1431 images belonging to 7 classes.\n",
            "\n",
            "Training Crop Model...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 14s/step - accuracy: 0.8248 - loss: 0.3498 - val_accuracy: 0.5625 - val_loss: 6.9238\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 13s/step - accuracy: 0.9746 - loss: 0.0806 - val_accuracy: 0.5625 - val_loss: 19.1281\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 8s/step - accuracy: 0.9859 - loss: 0.0584 - val_accuracy: 0.6313 - val_loss: 12.8522\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 5s/step - accuracy: 0.9814 - loss: 0.0422 - val_accuracy: 0.6469 - val_loss: 14.3670\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 5s/step - accuracy: 0.9771 - loss: 0.0755 - val_accuracy: 0.7937 - val_loss: 3.3355\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - accuracy: 0.9657 - loss: 0.0841 - val_accuracy: 0.6313 - val_loss: 13.8028\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - accuracy: 0.9865 - loss: 0.0407 - val_accuracy: 0.6094 - val_loss: 17.2781\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - accuracy: 0.9731 - loss: 0.1223 - val_accuracy: 0.7094 - val_loss: 11.6552\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.9677 - loss: 0.0848 - val_accuracy: 0.6594 - val_loss: 9.3553\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 4s/step - accuracy: 0.9910 - loss: 0.0357 - val_accuracy: 0.8188 - val_loss: 3.8393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Deficiency Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - accuracy: 0.5986 - loss: 1.2460 - val_accuracy: 0.3250 - val_loss: 9.2297\n",
            "Epoch 2/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - accuracy: 0.8536 - loss: 0.4097 - val_accuracy: 0.4031 - val_loss: 6.7421\n",
            "Epoch 3/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8691 - loss: 0.3923 - val_accuracy: 0.4375 - val_loss: 10.0225\n",
            "Epoch 4/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.8659 - loss: 0.3506 - val_accuracy: 0.4625 - val_loss: 8.5639\n",
            "Epoch 5/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.8729 - loss: 0.3811 - val_accuracy: 0.4875 - val_loss: 7.6182\n",
            "Epoch 6/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9130 - loss: 0.3138 - val_accuracy: 0.6031 - val_loss: 4.6353\n",
            "Epoch 7/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 989ms/step - accuracy: 0.9255 - loss: 0.2347 - val_accuracy: 0.6375 - val_loss: 4.7490\n",
            "Epoch 8/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 839ms/step - accuracy: 0.9342 - loss: 0.2164 - val_accuracy: 0.6531 - val_loss: 3.8329\n",
            "Epoch 9/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8869 - loss: 0.3464 - val_accuracy: 0.5500 - val_loss: 5.3705\n",
            "Epoch 10/10\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9265 - loss: 0.2166 - val_accuracy: 0.7844 - val_loss: 2.2061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Training Completed Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "import json\n",
        "\n",
        "# Paths\n",
        "data_dir = \"/content/drive/MyDrive/Dataset Crop/Crop/train\"\n",
        "\n",
        "# Image generator\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Training set\n",
        "train_gen = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(96, 96),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "# Validation set\n",
        "val_gen = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(96, 96),\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\",\n",
        "    subset=\"validation\"\n",
        ")\n",
        "\n",
        "# Save class indices (for mapping later)\n",
        "with open(\"deficiency_class_indices.json\", \"w\") as f:\n",
        "    json.dump(train_gen.class_indices, f)\n",
        "\n",
        "print(\"Class mapping:\", train_gen.class_indices)\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation=\"relu\", input_shape=(96,96,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation=\"relu\"),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation=\"relu\"),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_gen.num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Train\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=15\n",
        ")\n",
        "\n",
        "# Save\n",
        "model.save(\"deficiency_model.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XShxo4h0-0gL",
        "outputId": "4ce34d0b-5be2-4e86-bac9-c3426d16aca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5738 images belonging to 2 classes.\n",
            "Found 1433 images belonging to 2 classes.\n",
            "Class mapping: {'Mazie': 0, 'Wheat': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 1s/step - accuracy: 0.8450 - loss: 0.3390 - val_accuracy: 0.8667 - val_loss: 0.3474\n",
            "Epoch 2/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 470ms/step - accuracy: 0.9666 - loss: 0.1014 - val_accuracy: 0.9177 - val_loss: 0.2681\n",
            "Epoch 3/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 471ms/step - accuracy: 0.9759 - loss: 0.0722 - val_accuracy: 0.9142 - val_loss: 0.2499\n",
            "Epoch 4/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 468ms/step - accuracy: 0.9785 - loss: 0.0711 - val_accuracy: 0.9114 - val_loss: 0.3038\n",
            "Epoch 5/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 458ms/step - accuracy: 0.9844 - loss: 0.0501 - val_accuracy: 0.9225 - val_loss: 0.2825\n",
            "Epoch 6/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 475ms/step - accuracy: 0.9816 - loss: 0.0532 - val_accuracy: 0.9016 - val_loss: 0.3525\n",
            "Epoch 7/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 517ms/step - accuracy: 0.9824 - loss: 0.0496 - val_accuracy: 0.9316 - val_loss: 0.2696\n",
            "Epoch 8/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 536ms/step - accuracy: 0.9895 - loss: 0.0350 - val_accuracy: 0.9121 - val_loss: 0.3520\n",
            "Epoch 9/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 517ms/step - accuracy: 0.9878 - loss: 0.0375 - val_accuracy: 0.9302 - val_loss: 0.2595\n",
            "Epoch 10/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 522ms/step - accuracy: 0.9900 - loss: 0.0299 - val_accuracy: 0.8897 - val_loss: 0.5411\n",
            "Epoch 11/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 458ms/step - accuracy: 0.9804 - loss: 0.0599 - val_accuracy: 0.9316 - val_loss: 0.3331\n",
            "Epoch 12/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 463ms/step - accuracy: 0.9889 - loss: 0.0332 - val_accuracy: 0.9177 - val_loss: 0.2851\n",
            "Epoch 13/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 472ms/step - accuracy: 0.9893 - loss: 0.0369 - val_accuracy: 0.8765 - val_loss: 0.5775\n",
            "Epoch 14/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 453ms/step - accuracy: 0.9871 - loss: 0.0340 - val_accuracy: 0.8904 - val_loss: 0.4516\n",
            "Epoch 15/15\n",
            "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 522ms/step - accuracy: 0.9929 - loss: 0.0192 - val_accuracy: 0.9225 - val_loss: 0.2943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Load model\n",
        "model = tf.keras.models.load_model(\"deficiency_model.h5\")\n",
        "\n",
        "# Load class mapping (from training generator)\n",
        "with open(\"deficiency_class_indices.json\", \"r\") as f:\n",
        "    class_indices = json.load(f)\n",
        "\n",
        "# Reverse mapping: index -> class name\n",
        "idx_to_class = {v: k for k, v in class_indices.items()}\n",
        "\n",
        "# Prediction function\n",
        "def predict_crop_def(img_path):\n",
        "    # Load and preprocess image\n",
        "    img = image.load_img(img_path, target_size=(96,96))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # batch dimension\n",
        "\n",
        "    # Predict\n",
        "    preds = model.predict(img_array)\n",
        "    pred_idx = np.argmax(preds)\n",
        "    pred_class = idx_to_class[pred_idx]\n",
        "    confidence = np.max(preds)\n",
        "\n",
        "    # Split predicted class into crop and deficiency\n",
        "    # Assumes class names are in format: \"Crop_Deficiency\" or you can map manually\n",
        "    # If your generator uses subfolders as class names like \"Wheat_Brown_rust\", do this:\n",
        "    if \"_\" in pred_class:\n",
        "        crop_name, deficiency_name = pred_class.split(\"_\", 1)\n",
        "    else:\n",
        "        # fallback: try extracting folder names from image path\n",
        "        parts = os.path.normpath(img_path).split(os.sep)\n",
        "        crop_name = parts[-3]      # parent folder of deficiency\n",
        "        deficiency_name = parts[-2] # folder containing the image\n",
        "\n",
        "    return crop_name, deficiency_name, confidence\n",
        "\n",
        "# Example usage\n",
        "img_path = \"/content/drive/MyDrive/Dataset Crop/Crop/train/Mazie/Gray_Leaf_Spot/Corn_Gray_Spot (100).JPG\"\n",
        "crop, deficiency, conf = predict_crop_def(img_path)\n",
        "\n",
        "print(f\"ğŸŒ± Crop Identified: {crop}\")\n",
        "print(f\"âš ï¸ Deficiency Identified: {deficiency} (confidence {conf:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g60jfqf2GBGn",
        "outputId": "57b1ff34-4f84-4a81-d8c4-1c5f69ef5fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "ğŸŒ± Crop Identified: Mazie\n",
            "âš ï¸ Deficiency Identified: Gray_Leaf_Spot (confidence 1.00)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}